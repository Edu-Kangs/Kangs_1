{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1882037,
          "sourceType": "datasetVersion",
          "datasetId": 1120859
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Stroke Prediction | EDA | Hypothesis Testing",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edu-Kangs/Kangs_1/blob/main/Stroke_Prediction_%7C_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "fedesoriano_stroke_prediction_dataset_path = kagglehub.dataset_download('fedesoriano/stroke-prediction-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "1FSIY_Npq492",
        "outputId": "b7c76fc5-bd07-40c8-fcfb-f2db76aae513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/fedesoriano/stroke-prediction-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 67.4k/67.4k [00:00<00:00, 19.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:24.422009Z",
          "iopub.execute_input": "2024-11-22T06:30:24.42267Z",
          "iopub.status.idle": "2024-11-22T06:30:24.814631Z",
          "shell.execute_reply.started": "2024-11-22T06:30:24.422623Z",
          "shell.execute_reply": "2024-11-22T06:30:24.813621Z"
        },
        "id": "IWtOYDGwq494"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:24.815806Z",
          "iopub.execute_input": "2024-11-22T06:30:24.816211Z",
          "iopub.status.idle": "2024-11-22T06:30:34.706499Z",
          "shell.execute_reply.started": "2024-11-22T06:30:24.81618Z",
          "shell.execute_reply": "2024-11-22T06:30:34.705279Z"
        },
        "id": "aCrHdRZsq496",
        "outputId": "0e8bbf36-2cd5-4ded-c0c1-a09c5910b417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit_posthocs\n",
            "  Downloading scikit_posthocs-0.11.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (1.13.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (0.14.4)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (3.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->scikit_posthocs) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->scikit_posthocs) (1.17.0)\n",
            "Downloading scikit_posthocs-0.11.2-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: scikit_posthocs\n",
            "Successfully installed scikit_posthocs-0.11.2\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Utilities"
      ],
      "metadata": {
        "id": "CzYrfbFSq498"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import shapiro, normaltest\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (StandardScaler,\n",
        "                                   LabelEncoder,\n",
        "                                   OneHotEncoder)\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:34.70837Z",
          "iopub.execute_input": "2024-11-22T06:30:34.709261Z",
          "iopub.status.idle": "2024-11-22T06:30:35.465371Z",
          "shell.execute_reply.started": "2024-11-22T06:30:34.709212Z",
          "shell.execute_reply": "2024-11-22T06:30:35.464126Z"
        },
        "id": "FBcX78vmq49_"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.468764Z",
          "iopub.execute_input": "2024-11-22T06:30:35.46938Z",
          "iopub.status.idle": "2024-11-22T06:30:35.474257Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.469344Z",
          "shell.execute_reply": "2024-11-22T06:30:35.4732Z"
        },
        "id": "7BDAkqdbq4-C"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv(\"https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data?select=healthcare-dataset-stroke-data.csv\")\n",
        "df_raw.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.475695Z",
          "iopub.execute_input": "2024-11-22T06:30:35.476133Z",
          "iopub.status.idle": "2024-11-22T06:30:35.521551Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.476086Z",
          "shell.execute_reply": "2024-11-22T06:30:35.520388Z"
        },
        "id": "mAWgP2Kmq4-D",
        "outputId": "3dce94d5-f6f4-434b-ab72-326d5cf481d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-daac86ce21b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data?select=healthcare-dataset-stroke-data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the id column as we will be using the default index\n",
        "df_raw.drop(\"id\",axis = 1, inplace = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.52287Z",
          "iopub.execute_input": "2024-11-22T06:30:35.523225Z",
          "iopub.status.idle": "2024-11-22T06:30:35.52965Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.523192Z",
          "shell.execute_reply": "2024-11-22T06:30:35.528442Z"
        },
        "id": "ZGSNKWb-q4-E",
        "outputId": "601dacdf-2bd3-48a5-e5a7-aa460161942b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['id'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7068b61f2b4b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#dropping the id column as we will be using the default index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['id'] not found in axis\""
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "fA-L26C-q4-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.531094Z",
          "iopub.execute_input": "2024-11-22T06:30:35.531507Z",
          "iopub.status.idle": "2024-11-22T06:30:35.543668Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.531461Z",
          "shell.execute_reply": "2024-11-22T06:30:35.542604Z"
        },
        "id": "xdn9J-03q4-J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.info(show_counts=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.544944Z",
          "iopub.execute_input": "2024-11-22T06:30:35.5453Z",
          "iopub.status.idle": "2024-11-22T06:30:35.564179Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.545253Z",
          "shell.execute_reply": "2024-11-22T06:30:35.563063Z"
        },
        "id": "tkqjYib5q4-L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above that there are some missing values in \"bmi\" column. Lets say how to handle this.\n",
        "\n",
        "Possible ways to handle missing value :\n",
        "\n",
        "- Drop the rows with the missing value (bad because of data loss)\n",
        "- Missing value imputation with a measure of central tendancy (easy and quick)\n",
        "- Missing value imputation with a statistical/ML model (tedious sometimes but could be more effective)\n",
        "\n",
        "Alternative :\n",
        "Dont handle it and use a model that can inherently handle missing values"
      ],
      "metadata": {
        "id": "VNgScyw_q4-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many missing values are there ?\n",
        "print(\"No of Missing values in bmi column : \", df_raw['bmi'].isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.565527Z",
          "iopub.execute_input": "2024-11-22T06:30:35.565855Z",
          "iopub.status.idle": "2024-11-22T06:30:35.576103Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.565823Z",
          "shell.execute_reply": "2024-11-22T06:30:35.574967Z"
        },
        "id": "xrBxXJAPq4-P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, since I have plans to use linear models for baselining (stacking) and then build a neural network based final model, it kind of becomes necessary to handle the missing data. We will do this by building a machine learning model. But first lets see the distribution of this bmi."
      ],
      "metadata": {
        "id": "63R-aoesq4-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Histogram and KDE\n",
        "sns.histplot(df_raw['bmi'], kde=True, color='blue', bins=30)\n",
        "plt.title('Distribution of BMI', fontsize=16)\n",
        "plt.xlabel('BMI', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Statistical analysis to check normality\n",
        "print(\"Shapiro-Wilk Test:\")\n",
        "shapiro_test = shapiro(df_raw['bmi'])\n",
        "print(f\"Statistic: {shapiro_test.statistic:.4f}, p-value: {shapiro_test.pvalue:.4e}\")\n",
        "\n",
        "print(\"\\nD’Agostino and Pearson’s Test:\")\n",
        "dagostino_test = normaltest(df_raw['bmi'])\n",
        "print(f\"Statistic: {dagostino_test.statistic:.4f}, p-value: {dagostino_test.pvalue:.4e}\")\n",
        "\n",
        "# Interpretation\n",
        "if shapiro_test.pvalue > 0.05 and dagostino_test.pvalue > 0.05:\n",
        "    print(\"\\nThe BMI values appear to follow a normal distribution (Gaussian).\")\n",
        "else:\n",
        "    print(\"\\nThe BMI values do not follow a normal distribution and may be skewed.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:35.577575Z",
          "iopub.execute_input": "2024-11-22T06:30:35.577895Z",
          "iopub.status.idle": "2024-11-22T06:30:36.016299Z",
          "shell.execute_reply.started": "2024-11-22T06:30:35.577863Z",
          "shell.execute_reply": "2024-11-22T06:30:36.015231Z"
        },
        "id": "AkXt-n-kq4-S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I really didnt need to do the shapiro-wilk test and all, its pretty evident from the distribution, that it is not exactly gaussian, but it is not too strongly skewed either. Hence maybe replacing by a measure of central tendency like mean, wont be too bad. But I want to go overboard and make it a model for imputation. We will use a decision tree model for now since that seems to have given other kaggleres the best results. Later we will try a KNN too."
      ],
      "metadata": {
        "id": "7B0THqlqq4-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the wonderful piece of code from Thomas Konstantin's Notebook\n",
        "DT_bmi_pipe = Pipeline( steps=[\n",
        "                               ('scale',StandardScaler()),\n",
        "                               ('lr',DecisionTreeRegressor(random_state=42))\n",
        "                              ])\n",
        "X = df_raw[['age','gender','bmi']].copy()\n",
        "X.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n",
        "\n",
        "Missing = X[X.bmi.isna()]\n",
        "X = X[~X.bmi.isna()]\n",
        "Y = X.pop('bmi')\n",
        "DT_bmi_pipe.fit(X,Y)\n",
        "predicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),index=Missing.index)\n",
        "df_raw.loc[Missing.index,'bmi'] = predicted_bmi"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:36.017604Z",
          "iopub.execute_input": "2024-11-22T06:30:36.017945Z",
          "iopub.status.idle": "2024-11-22T06:30:36.045511Z",
          "shell.execute_reply.started": "2024-11-22T06:30:36.017886Z",
          "shell.execute_reply": "2024-11-22T06:30:36.044367Z"
        },
        "id": "zvipDovBq4-U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if missing values are imputed\n",
        "df_raw.bmi.isnull().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:36.04708Z",
          "iopub.execute_input": "2024-11-22T06:30:36.047414Z",
          "iopub.status.idle": "2024-11-22T06:30:36.054082Z",
          "shell.execute_reply.started": "2024-11-22T06:30:36.047382Z",
          "shell.execute_reply": "2024-11-22T06:30:36.053122Z"
        },
        "id": "DVYVVV5Aq4-V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extensive EDA"
      ],
      "metadata": {
        "id": "tdEfI1r0q4-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets explore the target variable \"stroke\"\n",
        "# Note - 1 represents stroke happening & 0 represents no Stroke\n",
        "\n",
        "\n",
        "stroke_counts = df_raw[\"stroke\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "stroke_counts.plot.pie(autopct='%1.1f%%', startangle=90, labels=stroke_counts.index, colors=['skyblue', 'salmon'])\n",
        "plt.title('Distribution of Stroke Variable', fontsize=16)\n",
        "plt.ylabel('')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:36.058075Z",
          "iopub.execute_input": "2024-11-22T06:30:36.058419Z",
          "iopub.status.idle": "2024-11-22T06:30:36.169014Z",
          "shell.execute_reply.started": "2024-11-22T06:30:36.058377Z",
          "shell.execute_reply": "2024-11-22T06:30:36.168019Z"
        },
        "id": "c1MWrTFVq4-W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can clearly see from the diagram, the target variable is highly skewed which basically directs me towards using SMOTE to resolve this issue. Lets see."
      ],
      "metadata": {
        "id": "Sbj_RLLOq4-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling out lists of all categorical and numerical variables for easier interpretation\n",
        "\n",
        "num_vars = ['age','avg_glucose_level','bmi']\n",
        "cat_vars = [var for var in df_raw.columns if var not in ['stroke','id'] and var not in num_vars]\n",
        "\n",
        "target = 'stroke'\n",
        "\n",
        "print(\"Categorical Variables :\", cat_vars)\n",
        "print(\"Numerical Variables :\", num_vars)\n",
        "print(\"Target Variable :\", target)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:36.17024Z",
          "iopub.execute_input": "2024-11-22T06:30:36.170666Z",
          "iopub.status.idle": "2024-11-22T06:30:36.178089Z",
          "shell.execute_reply.started": "2024-11-22T06:30:36.17062Z",
          "shell.execute_reply": "2024-11-22T06:30:36.177046Z"
        },
        "id": "ZmAt-izHq4-X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at the numerical variables first"
      ],
      "metadata": {
        "id": "BssJ1edBq4-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate Analysis"
      ],
      "metadata": {
        "id": "j8P5Ao4cq4-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot frequency distribution for each numerical variable\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, var in enumerate(num_vars, 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "    sns.kdeplot(data=df_raw[var], shade=True, color=\"teal\")\n",
        "    plt.title(f'Frequency Distribution of {var}', fontsize=14)\n",
        "    plt.xlabel(var, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:36.179193Z",
          "iopub.execute_input": "2024-11-22T06:30:36.179576Z",
          "iopub.status.idle": "2024-11-22T06:30:37.227239Z",
          "shell.execute_reply.started": "2024-11-22T06:30:36.179532Z",
          "shell.execute_reply": "2024-11-22T06:30:37.226125Z"
        },
        "id": "mSlYrbBRq4-Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like avg_glucose_level and bmi have positive skews"
      ],
      "metadata": {
        "id": "wSOw1gHXq4-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot frequency distributions grouped by 'stroke'\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, var in enumerate(num_vars, 1):\n",
        "    plt.subplot(3, 2, i)  # Create a grid of 3 rows and 2 columns\n",
        "    sns.kdeplot(\n",
        "        data=df_raw, x=var, hue='stroke',\n",
        "        shade=True, alpha=0.5\n",
        "    )\n",
        "    plt.title(f'Distribution of {var} by Stroke', fontsize=14)\n",
        "    plt.xlabel(var, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:37.228553Z",
          "iopub.execute_input": "2024-11-22T06:30:37.228891Z",
          "iopub.status.idle": "2024-11-22T06:30:38.290657Z",
          "shell.execute_reply.started": "2024-11-22T06:30:37.22886Z",
          "shell.execute_reply": "2024-11-22T06:30:38.289581Z"
        },
        "id": "KdL2jiGHq4-a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graphs give a good idea of the distrbution of numerical variables w.r.t the stroke variable. But since the data is highly skewed for stroke, hence its tough to get an idea.\n",
        "We will be plotting these graphs again below, but this time each group (i.e stroke =1 and stroke = 0) will be independently normalised and then plotted."
      ],
      "metadata": {
        "id": "dSlhwVIyq4-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, var in enumerate(num_vars, 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "    sns.kdeplot(\n",
        "        data=df_raw, x=var, hue='stroke',\n",
        "        shade=True, alpha=0.5, common_norm=False\n",
        "    )\n",
        "    plt.title(f'Distribution of {var} by Stroke', fontsize=14)\n",
        "    plt.xlabel(var, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:38.292156Z",
          "iopub.execute_input": "2024-11-22T06:30:38.292555Z",
          "iopub.status.idle": "2024-11-22T06:30:39.364599Z",
          "shell.execute_reply.started": "2024-11-22T06:30:38.292514Z",
          "shell.execute_reply": "2024-11-22T06:30:39.361946Z"
        },
        "id": "YuUsXCnwq4-b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kind of telling us straight up that age is a very important factor for stroke prediction - since as age is increasing past 40 there is a sharp increase in positive strokes. We can really say much about the other 2 features as of now."
      ],
      "metadata": {
        "id": "sqFrYQhRq4-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets perform a quick correlation analysis to strengthen our beliefs"
      ],
      "metadata": {
        "id": "EeLzQtHXq4-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting a correlation heatmap\n",
        "corr_data = df_raw[['age', 'avg_glucose_level', 'bmi', 'stroke']]\n",
        "\n",
        "correlation_matrix = corr_data.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
        "plt.title('Correlation Heatmap', fontsize=16)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:39.366278Z",
          "iopub.execute_input": "2024-11-22T06:30:39.366736Z",
          "iopub.status.idle": "2024-11-22T06:30:39.734212Z",
          "shell.execute_reply.started": "2024-11-22T06:30:39.36668Z",
          "shell.execute_reply": "2024-11-22T06:30:39.732978Z"
        },
        "id": "2acspWiCq4-c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is again proving that in comparison to bmi and avg_glucose_level, age happens to have a stronger postive correlation with stroke"
      ],
      "metadata": {
        "id": "e4n41hvmq4-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame to place stroke=1 entries last (drawn on top)\n",
        "df_raw_sorted = df_raw.sort_values(by='stroke')\n",
        "\n",
        "# Create subplots for all combinations of 'age', 'avg_glucose_level', 'bmi'\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Variable combinations - (x-axis, y-axis)\n",
        "var_combinations = [('bmi', 'age'), ('avg_glucose_level','bmi'), ('avg_glucose_level','age')]\n",
        "\n",
        "for i, (x_var, y_var) in enumerate(var_combinations):\n",
        "    sns.scatterplot(\n",
        "        data=df_raw_sorted,\n",
        "        x=x_var,\n",
        "        y=y_var,\n",
        "        hue='stroke',\n",
        "        alpha=0.7,\n",
        "        s=100,\n",
        "        ax=axes[i]\n",
        "    )\n",
        "    axes[i].set_title(f'{x_var} vs {y_var} by Stroke', fontsize=14)\n",
        "    axes[i].set_xlabel(x_var, fontsize=12)\n",
        "    axes[i].set_ylabel(y_var, fontsize=12)\n",
        "    axes[i].grid(True)\n",
        "    axes[i].legend(title='Stroke', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:39.735751Z",
          "iopub.execute_input": "2024-11-22T06:30:39.736211Z",
          "iopub.status.idle": "2024-11-22T06:30:41.20303Z",
          "shell.execute_reply.started": "2024-11-22T06:30:39.736163Z",
          "shell.execute_reply": "2024-11-22T06:30:41.202002Z"
        },
        "id": "oqEBYVbfq4-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again reiterating on the same fact that age is clearly way more important than avg_glucose_level and bmi. However these graphs do some kind of a relationship between bmi and glucose_level as you can see that both bmi and glucose when low and when high have more chances of stroke. Chances of stroke thin out in the middle region of the second plot."
      ],
      "metadata": {
        "id": "cb8AofM9q4-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets move on to categorical features now"
      ],
      "metadata": {
        "id": "HFnsrxrHq4-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gender\n",
        "gender_dist = df_raw[\"gender\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "gender_dist.plot.pie(autopct='%1.1f%%', startangle=90, labels=gender_dist.index, colors=['skyblue', 'salmon'])\n",
        "plt.title('Distribution of Gender Variable', fontsize=16)\n",
        "plt.ylabel('')\n",
        "plt.show()\n",
        "\n",
        "print(gender_dist)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:41.204512Z",
          "iopub.execute_input": "2024-11-22T06:30:41.204963Z",
          "iopub.status.idle": "2024-11-22T06:30:41.323706Z",
          "shell.execute_reply.started": "2024-11-22T06:30:41.204867Z",
          "shell.execute_reply": "2024-11-22T06:30:41.322371Z"
        },
        "id": "yb_VHar0q4-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the count of each stroke value per gender\n",
        "gender_stroke_counts = df_raw.groupby(['gender', 'stroke']).size().unstack(fill_value=0)\n",
        "\n",
        "# Plot stacked bar chart\n",
        "plt.figure(figsize=(5, 4))\n",
        "gender_stroke_counts.plot(kind='bar', stacked=True, color=['skyblue', 'salmon'], figsize=(5, 4))\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Stroke Distribution by Gender (Stacked)', fontsize=16)\n",
        "plt.xlabel('Gender', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.legend(['No Stroke (0)', 'Stroke (1)'], title='Stroke', fontsize=12)\n",
        "plt.xticks(rotation=0, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:41.325441Z",
          "iopub.execute_input": "2024-11-22T06:30:41.326406Z",
          "iopub.status.idle": "2024-11-22T06:30:41.605583Z",
          "shell.execute_reply.started": "2024-11-22T06:30:41.326349Z",
          "shell.execute_reply": "2024-11-22T06:30:41.604355Z"
        },
        "id": "gGtgQN7Xq4-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that gender doesnt seem to be a contributing factor to stroke since the distributions are quite similar. This should probably show up when we are doing a correlation heatmap later on."
      ],
      "metadata": {
        "id": "4YleKPnaq4-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_stroke = df_raw[df_raw['stroke'] == 0]\n",
        "only_stroke = df_raw[df_raw['stroke'] == 1]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:41.607182Z",
          "iopub.execute_input": "2024-11-22T06:30:41.608017Z",
          "iopub.status.idle": "2024-11-22T06:30:41.615825Z",
          "shell.execute_reply.started": "2024-11-22T06:30:41.607964Z",
          "shell.execute_reply": "2024-11-22T06:30:41.614721Z"
        },
        "id": "4DnFVnJkq4-f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def bivariate_analysis(column_name, ax):\n",
        "    # Calculate counts for stroke = 1\n",
        "    positive = only_stroke[column_name].value_counts().reset_index()\n",
        "    positive.columns = [column_name, \"count\"]\n",
        "    positive[\"Percentage\"] = positive[\"count\"] / positive[\"count\"].sum() * 100\n",
        "\n",
        "    # Calculate counts for stroke = 0\n",
        "    negative = no_stroke[column_name].value_counts().reset_index()\n",
        "    negative.columns = [column_name, \"count\"]\n",
        "    negative[\"Percentage\"] = negative[\"count\"] / negative[\"count\"].sum() * 100\n",
        "\n",
        "    # Plot horizontal bar chart on the provided axis\n",
        "    ax.barh(positive[column_name], positive[\"Percentage\"], color=\"teal\", label='Stroke = 1', height=0.7)\n",
        "    ax.barh(negative[column_name], negative[\"Percentage\"], color=\"blue\", label='Stroke = 0', height=0.3)\n",
        "\n",
        "    # Format x-axis as percentage\n",
        "    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "    ax.xaxis.set_major_locator(mtick.MultipleLocator(10))\n",
        "\n",
        "    # Add titles and labels\n",
        "    ax.set_title(f\"{column_name} Distribution by Stroke\", fontsize=10)\n",
        "    ax.set_xlabel('Percentage', fontsize=8)\n",
        "    ax.set_ylabel(column_name, fontsize=8)\n",
        "    ax.legend(title='Stroke Status', fontsize=8, loc='upper right')\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.6, zorder=0)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:41.617563Z",
          "iopub.execute_input": "2024-11-22T06:30:41.618334Z",
          "iopub.status.idle": "2024-11-22T06:30:41.628303Z",
          "shell.execute_reply.started": "2024-11-22T06:30:41.618277Z",
          "shell.execute_reply": "2024-11-22T06:30:41.627267Z"
        },
        "id": "Z3aStXGPq4-f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, var in enumerate(cat_vars, 1):\n",
        "    plt.subplot(4, 2, i)\n",
        "    ax = plt.gca()\n",
        "    bivariate_analysis(var, ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:41.629654Z",
          "iopub.execute_input": "2024-11-22T06:30:41.630215Z",
          "iopub.status.idle": "2024-11-22T06:30:43.073271Z",
          "shell.execute_reply.started": "2024-11-22T06:30:41.630168Z",
          "shell.execute_reply": "2024-11-22T06:30:43.072003Z"
        },
        "id": "djvhhDegq4-f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference:\n",
        "\n",
        "Quite a lot of inferences here. Lets go one by one.\n",
        "- **Gender** : Men seem to be more prone to stroke. But the difference is not big enough to conclude anything.\n",
        "- **Hypertension** : We can clearly see that people with hypertension = 1 is clearly more prone to strokes.\n",
        "- **heart_disease** : People with heart disease = 1 is more prone to stroke. Kind of a no brainer.\n",
        "- **ever_married** : According to this married people seem to be more prone to strokes. XD\n",
        "- **work_type** : Children barely ever get strokes. Cant say much about private jobs. But self employed people have had more strokes (relatively)\n",
        "- **Residence_type** : Urban people have had a relatively more strokes in comparison to rural. But again the difference isnt huge so it might be a sampling issue. Cant draw any strong conclusions here.\n",
        "- **smoking_status** : This one is a bit weird. Its surprising that people who smoke currently have near about equal distrbution of strokes and no strokes. But in the group of formerly smoked, we see a considerable difference between strokes and no strokes with chances of strokes being much higher. This is a bit difficult to understand. We would have to do a detailed multivariate analysis with other features like age, heart_disease, work_type etc to actually draw a proper answer to this question"
      ],
      "metadata": {
        "id": "UX1wGv5zq4-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis Testing"
      ],
      "metadata": {
        "id": "mmo42I02q4-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok maybe I am over complicating at this point, but I am very curious about the smoking_status feature. I have a strong intuition that smoking_status is a confounding variable and it has different distribution of other variables (like age, bmi etc) across the different smoking groups. For example - It could be possible that most of the former smokers have higher age which is the primary reason for the strokes, not the fact that they are former smokers.\n",
        "\n",
        "If this comes out to be true, we might have to either do some kind of dimensionality reduction with PCA (i dont want to get into that) or merge smoking groups with other variables and not keep it as an independent feature (I like this much better).\n",
        "\n",
        "Hence lets do a **Hypothesis Testing **\n",
        "\n",
        "- Null Hypothesis (H₀): There is no difference in the distribution of these variables between smoking groups.\n",
        "  \n",
        "- Alternative Hypothesis (H₁): There is a significant difference in the distributions."
      ],
      "metadata": {
        "id": "VCodsYweq4-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.smoking_status.value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:43.074765Z",
          "iopub.execute_input": "2024-11-22T06:30:43.075267Z",
          "iopub.status.idle": "2024-11-22T06:30:43.086068Z",
          "shell.execute_reply.started": "2024-11-22T06:30:43.075217Z",
          "shell.execute_reply": "2024-11-22T06:30:43.084976Z"
        },
        "id": "nqq0vtFFq4-h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define groups\n",
        "group_smokes = df_raw[df_raw['smoking_status'] == 'smokes']\n",
        "group_formerly_smoked = df_raw[df_raw['smoking_status'] == 'formerly smoked']\n",
        "group_never_smoked = df_raw[df_raw['smoking_status'] == 'never smoked']\n",
        "group_unknown = df_raw[df_raw['smoking_status'] == 'Unknown']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:43.087382Z",
          "iopub.execute_input": "2024-11-22T06:30:43.087709Z",
          "iopub.status.idle": "2024-11-22T06:30:43.10715Z",
          "shell.execute_reply.started": "2024-11-22T06:30:43.087679Z",
          "shell.execute_reply": "2024-11-22T06:30:43.105993Z"
        },
        "id": "05FZXTXtq4-i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#running Shapiro-Wilk test\n",
        "# Test normality for 'age' in each group\n",
        "for group, name in zip([group_smokes, group_formerly_smoked, group_never_smoked, group_unknown],\n",
        "                        ['smokes', 'formerly smoked', 'never smoked', 'Unknown']):\n",
        "    stat, p = stats.shapiro(group['age'])\n",
        "    print(f\"{name}: p-value = {p:.4f} {'(Normal)' if p > 0.05 else '(Not Normal)'}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:43.108587Z",
          "iopub.execute_input": "2024-11-22T06:30:43.10898Z",
          "iopub.status.idle": "2024-11-22T06:30:43.120582Z",
          "shell.execute_reply.started": "2024-11-22T06:30:43.10894Z",
          "shell.execute_reply": "2024-11-22T06:30:43.119445Z"
        },
        "id": "odZuyrjlq4-i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the feature is NOT Normal we cannot use **ANOVA**. Instead we will use **Kruskal-Wallis**."
      ],
      "metadata": {
        "id": "QIj6JJM1q4-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kruskal-Wallis Test to compare age across the groups\n",
        "stat, p = stats.kruskal(group_smokes['age'], group_formerly_smoked['age'],\n",
        "                        group_never_smoked['age'], group_unknown['age'])\n",
        "print(f\"Kruskal-Wallis Test p-value = {p:.4f} {'(Significant)' if p < 0.05 else '(Not Significant)'}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:43.121783Z",
          "iopub.execute_input": "2024-11-22T06:30:43.122184Z",
          "iopub.status.idle": "2024-11-22T06:30:43.13589Z",
          "shell.execute_reply.started": "2024-11-22T06:30:43.122144Z",
          "shell.execute_reply": "2024-11-22T06:30:43.134594Z"
        },
        "id": "fSwEEL7nq4-j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the Kruskal-Wallis Test p-value is 0.0000, it indicates that there are significant differences in the distribution of age across the different smoking groups (smokes, formerly smoked, never smoked, and unknown). This means that at least one of the smoking groups has a different distribution of age compared to the others. **Hence Null Hypothesis is Rejected.**"
      ],
      "metadata": {
        "id": "0ChSV7P9q4-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a DataFrame with 'age' and 'smoking_status' columns\n",
        "data_all = pd.concat([group_smokes['age'], group_formerly_smoked['age'],\n",
        "                      group_never_smoked['age'], group_unknown['age']], axis=0)\n",
        "labels = ['smokes']*len(group_smokes) + ['formerly smoked']*len(group_formerly_smoked) + \\\n",
        "         ['never smoked']*len(group_never_smoked) + ['Unknown']*len(group_unknown)\n",
        "\n",
        "# Combine data into a DataFrame\n",
        "df_combined = pd.DataFrame({'age': data_all, 'smoking_status': labels})\n",
        "\n",
        "# Perform Dunn's Test with Bonferroni correction\n",
        "posthoc_result = posthoc_dunn(df_combined, val_col='age', group_col='smoking_status', p_adjust='bonferroni')\n",
        "\n",
        "# Print the result\n",
        "print(posthoc_result)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:43.137291Z",
          "iopub.execute_input": "2024-11-22T06:30:43.137617Z",
          "iopub.status.idle": "2024-11-22T06:30:43.161802Z",
          "shell.execute_reply.started": "2024-11-22T06:30:43.137586Z",
          "shell.execute_reply": "2024-11-22T06:30:43.160646Z"
        },
        "id": "-HxyoIVpq4-l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK I did a lot of tests. Some of them I have never heard of before, like the Dunn test. But to explain simply :\n",
        "We know that null hypothesis is rejected after kruskal-willis but we dont know which group inside smoking_status is the problem group. For that we are doing the Dunn Test.\n",
        "\n",
        "We will interpret the above results as follows :\n",
        "\n",
        "- **Unknown vs Formerly smoked**: The p-value is extremely small (2.042780e-137), which indicates a very significant difference between the Unknown and Formerly smoked groups.\n",
        "\n",
        "- **Never smoked vs Smokes**: The p-value is 1.000000e+00, meaning there is no significant difference between the Never smoked and Smokes groups, at least in terms of the age distribution.\n",
        "\n",
        "- **Smokes vs Formerly smoked**: The p-value (2.184769e-13) is very small, suggesting a significant difference between these two groups.\n",
        "\n",
        "Note : the unkown group in general has very different distribution from all the other 3 groups followed by formerly smoked. This is enough to conclude that smoking_status is pretty much acting as a counfounfing variable."
      ],
      "metadata": {
        "id": "1Ps_zQMTq4-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Big question now -\n",
        "\n",
        "**What is a Confounding Variable?**\n",
        "A confounding variable is one that:\n",
        "Affects both the independent variable (predictor) and the dependent variable (outcome).\n",
        "Its effect can distort or bias the relationship between the independent and dependent variables."
      ],
      "metadata": {
        "id": "PZ2xGpZoq4-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Keep the Confounder?\n",
        "\n",
        "To better understand this, consider a simple analogy:\n",
        "\n",
        "Without adjusting for smoking_status: You might find a strong relationship between age and stroke, but this relationship could be influenced by the fact that smokers (who might be younger) are also at risk of stroke.\n",
        "\n",
        "With adjusting for smoking_status: Now, you can isolate the effect of age on stroke risk while controlling for smoking status (i.e., the model knows whether someone is a smoker, former smoker, or non-smoker and accounts for that in the prediction).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iKOqrT5Cq4-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we finish off the EDA, lets create a final correlation heatmap including all features, just to finalise the decisions on whether we need to create interactions between multiple variables or not"
      ],
      "metadata": {
        "id": "OV3Q3efFq4-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:30:43.163156Z",
          "iopub.execute_input": "2024-11-22T06:30:43.163455Z",
          "iopub.status.idle": "2024-11-22T06:30:43.178802Z",
          "shell.execute_reply.started": "2024-11-22T06:30:43.163426Z",
          "shell.execute_reply": "2024-11-22T06:30:43.177814Z"
        },
        "id": "J6ZR7uIEq4-o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df_raw.copy()\n",
        "# feature log transformations\n",
        "\n",
        "df_copy['age'] = df_copy['age'].apply(lambda x: np.log(x+10)*3)\n",
        "df_copy['avg_glucose_level'] = df_copy['avg_glucose_level'].apply(lambda x: np.log(x+10)*2)\n",
        "df_copy['bmi'] = df_copy['bmi'].apply(lambda x: np.log(x+10)*2)\n",
        "\n",
        "\n",
        "\n",
        "# preprocessing - label enconding and numerical value scaling\n",
        "ohe = OneHotEncoder()\n",
        "le = LabelEncoder()\n",
        "\n",
        "## label encoding of ordinal categorical features\n",
        "for col in df_copy.columns:\n",
        "    df_copy[col] = le.fit_transform(df_copy[col])\n",
        "\n",
        "cols = df_copy.columns\n",
        "\n",
        "\n",
        "# correlation map for all the features\n",
        "df_corr = df_copy.corr()\n",
        "mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (8,8))\n",
        "fig.patch.set_facecolor('#f6f5f5')\n",
        "ax.set_facecolor('#f6f5f5')\n",
        "\n",
        "mask = mask[1:, :-1]\n",
        "corr = df_corr.iloc[1:,:-1].copy()\n",
        "\n",
        "\n",
        "colors = ['#f6f5f5','#512b58','#fe346e']\n",
        "colormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
        "\n",
        "# plot heatmap\n",
        "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\",cmap = colormap,\n",
        "           vmin=-0.15, vmax=0.5, cbar_kws={\"shrink\": .5, }, ax = ax, cbar = False,\n",
        "           linewidth = 1,linecolor = '#f6f5f5', square = True,annot_kws = {'font':'serif', 'size':10, 'color':'black'} )\n",
        "# yticks\n",
        "ax.tick_params(axis = 'y', rotation=0)\n",
        "xticks = ['Gender', 'Age','Hyper tension', 'Heart Disease', 'Marriage', 'Work', 'Residence', 'Glucose Level', 'BMI', 'Smoking Status','Stroke','BMI Cat','Age Cat']\n",
        "yticks = ['Gender', 'Age','Hyper tension', 'Heart Disease', 'Marriage', 'Work', 'Residence', 'Glucose Level', 'BMI', 'Smoking Status','Stroke','BMI Cat','Age Cat']\n",
        "\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T06:32:40.718281Z",
          "iopub.execute_input": "2024-11-22T06:32:40.718674Z",
          "iopub.status.idle": "2024-11-22T06:32:41.186047Z",
          "shell.execute_reply.started": "2024-11-22T06:32:40.718637Z",
          "shell.execute_reply": "2024-11-22T06:32:41.184987Z"
        },
        "id": "AszIE091q4-p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few key observations from the above heatmap:\n",
        "\n",
        "1. Age is ofc the most prominant predictor of strokes.\n",
        "2. Gender barely has any predicting capabilities. I am just gonna drop it.\n",
        "3. ever_married is interesting. We see the bright pink color showing that ever_married is highly correlated with age. Which basically means the more your age the more the chances of you being married. Now remember we found that married people have significant more strokes. This is basically because of its strong relation with age and not because of marriage itself. Hence I am gonna drop this too.\n"
      ],
      "metadata": {
        "id": "HE1HEHrjq4-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have decided to do data prep, baselining and stacking in a new notebook. So lets end this notebook here with just extensive EDA.\n",
        "\n",
        "############################### The End ###############################"
      ],
      "metadata": {
        "id": "m0KUf7MTq4-q"
      }
    }
  ]
}